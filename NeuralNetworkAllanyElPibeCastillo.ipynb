{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "import matplotlib as mpl\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Karpathy 1. fix random seed\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATT_FILE_NAME = \"FootballPlayerPreparedCleanAttributes.csv\"\n",
    "ONE_HOT_ENCODED_CLASSES_FILE_NAME = \"FootballPlayerOneHotEncodedClasses.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = pd.read_csv(ATT_FILE_NAME)\n",
    "target = pd.read_csv(ONE_HOT_ENCODED_CLASSES_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Crossing</th>\n",
       "      <th>HeadingAccuracy</th>\n",
       "      <th>ShortPassing</th>\n",
       "      <th>Volleys</th>\n",
       "      <th>Dribbling</th>\n",
       "      <th>Curve</th>\n",
       "      <th>FKAccuracy</th>\n",
       "      <th>LongPassing</th>\n",
       "      <th>BallControl</th>\n",
       "      <th>Reactions</th>\n",
       "      <th>ShotPower</th>\n",
       "      <th>Stamina</th>\n",
       "      <th>LongShots</th>\n",
       "      <th>Aggression</th>\n",
       "      <th>Positioning</th>\n",
       "      <th>Vision</th>\n",
       "      <th>Composure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16122.000000</td>\n",
       "      <td>16122.000000</td>\n",
       "      <td>16122.000000</td>\n",
       "      <td>16122.000000</td>\n",
       "      <td>16122.000000</td>\n",
       "      <td>16122.000000</td>\n",
       "      <td>16122.000000</td>\n",
       "      <td>16122.000000</td>\n",
       "      <td>16122.000000</td>\n",
       "      <td>16122.000000</td>\n",
       "      <td>16122.000000</td>\n",
       "      <td>16122.000000</td>\n",
       "      <td>16122.000000</td>\n",
       "      <td>16122.000000</td>\n",
       "      <td>16122.000000</td>\n",
       "      <td>16122.000000</td>\n",
       "      <td>16122.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.053537</td>\n",
       "      <td>0.064942</td>\n",
       "      <td>0.168294</td>\n",
       "      <td>-0.081373</td>\n",
       "      <td>0.122654</td>\n",
       "      <td>-0.030012</td>\n",
       "      <td>-0.132537</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>0.076091</td>\n",
       "      <td>0.098800</td>\n",
       "      <td>0.125965</td>\n",
       "      <td>0.169170</td>\n",
       "      <td>-0.025506</td>\n",
       "      <td>0.136145</td>\n",
       "      <td>0.043417</td>\n",
       "      <td>0.060219</td>\n",
       "      <td>0.036497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.344606</td>\n",
       "      <td>0.293820</td>\n",
       "      <td>0.268085</td>\n",
       "      <td>0.369549</td>\n",
       "      <td>0.301492</td>\n",
       "      <td>0.364944</td>\n",
       "      <td>0.358826</td>\n",
       "      <td>0.334822</td>\n",
       "      <td>0.282066</td>\n",
       "      <td>0.233806</td>\n",
       "      <td>0.326638</td>\n",
       "      <td>0.324097</td>\n",
       "      <td>0.379027</td>\n",
       "      <td>0.352347</td>\n",
       "      <td>0.349409</td>\n",
       "      <td>0.314939</td>\n",
       "      <td>0.274291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.195122</td>\n",
       "      <td>-0.139241</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>-0.012048</td>\n",
       "      <td>-0.325301</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.189189</td>\n",
       "      <td>-0.070423</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>-0.086420</td>\n",
       "      <td>-0.014493</td>\n",
       "      <td>-0.301205</td>\n",
       "      <td>-0.097561</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.146341</td>\n",
       "      <td>-0.162162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>0.205479</td>\n",
       "      <td>-0.075000</td>\n",
       "      <td>0.180723</td>\n",
       "      <td>-0.012048</td>\n",
       "      <td>-0.190476</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.098592</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>0.160494</td>\n",
       "      <td>0.188406</td>\n",
       "      <td>0.036145</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.054054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.265823</td>\n",
       "      <td>0.342466</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.325301</td>\n",
       "      <td>0.253012</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.243243</td>\n",
       "      <td>0.267606</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.358025</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.277108</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.292683</td>\n",
       "      <td>0.243243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Crossing  HeadingAccuracy  ShortPassing       Volleys  \\\n",
       "count  16122.000000     16122.000000  16122.000000  16122.000000   \n",
       "mean       0.053537         0.064942      0.168294     -0.081373   \n",
       "std        0.344606         0.293820      0.268085      0.369549   \n",
       "min       -1.000000        -1.000000     -1.000000     -1.000000   \n",
       "25%       -0.195122        -0.139241      0.013699     -0.375000   \n",
       "50%        0.097561         0.088608      0.205479     -0.075000   \n",
       "75%        0.317073         0.265823      0.342466      0.200000   \n",
       "max        1.000000         1.000000      1.000000      1.000000   \n",
       "\n",
       "          Dribbling         Curve    FKAccuracy   LongPassing   BallControl  \\\n",
       "count  16122.000000  16122.000000  16122.000000  16122.000000  16122.000000   \n",
       "mean       0.122654     -0.030012     -0.132537      0.002751      0.076091   \n",
       "std        0.301492      0.364944      0.358826      0.334822      0.282066   \n",
       "min       -1.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "25%       -0.012048     -0.325301     -0.428571     -0.189189     -0.070423   \n",
       "50%        0.180723     -0.012048     -0.190476      0.054054      0.098592   \n",
       "75%        0.325301      0.253012      0.142857      0.243243      0.267606   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "          Reactions     ShotPower       Stamina     LongShots    Aggression  \\\n",
       "count  16122.000000  16122.000000  16122.000000  16122.000000  16122.000000   \n",
       "mean       0.098800      0.125965      0.169170     -0.025506      0.136145   \n",
       "std        0.233806      0.326638      0.324097      0.379027      0.352347   \n",
       "min       -1.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "25%       -0.066667     -0.086420     -0.014493     -0.301205     -0.097561   \n",
       "50%        0.093333      0.160494      0.188406      0.036145      0.170732   \n",
       "75%        0.253333      0.358025      0.391304      0.277108      0.390244   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "        Positioning        Vision     Composure  \n",
       "count  16122.000000  16122.000000  16122.000000  \n",
       "mean       0.043417      0.060219      0.036497  \n",
       "std        0.349409      0.314939      0.274291  \n",
       "min       -1.000000     -1.000000     -1.000000  \n",
       "25%       -0.166667     -0.146341     -0.162162  \n",
       "50%        0.095238      0.097561      0.054054  \n",
       "75%        0.285714      0.292683      0.243243  \n",
       "max        1.000000      1.000000      1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Poor:[46.0, 62.0]</th>\n",
       "      <th>Interm:[63.0, 66.0]</th>\n",
       "      <th>Good:[67.0, 71.0]</th>\n",
       "      <th>Excel:[72.0, 94.0]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Poor:[46.0, 62.0]  Interm:[63.0, 66.0]  Good:[67.0, 71.0]  \\\n",
       "0                0.0                  1.0                0.0   \n",
       "1                0.0                  0.0                1.0   \n",
       "2                1.0                  0.0                0.0   \n",
       "3                0.0                  0.0                1.0   \n",
       "4                0.0                  1.0                0.0   \n",
       "\n",
       "   Excel:[72.0, 94.0]  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clases = np.argmax(target.to_numpy(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4296, 3868, 4353, 3605], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(clases) # contar cuantos elemntos tengo de cada clase en el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 16122. C1 4296, c2 3868, c3 4353, c4 3605\n"
     ]
    }
   ],
   "source": [
    "c1, c2,c3,c4= np.bincount(clases)\n",
    "total = c1+c2+c3+c4\n",
    "print(f\"Total: {total}. C1 {c1}, c2 {c2}, c3 {c3}, c4 {c4}\") #lo mismo de arriba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_rem,y_train,y_rem = train_test_split(att,target,train_size=0.8)\n",
    "X_valid,X_test,y_valid,y_test = train_test_split(X_rem,y_rem,train_size=0.5)\n",
    "#80 % train   --- 10% val 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 12897. C1 3455, c2 3086, c3 3479, c4 2877\n"
     ]
    }
   ],
   "source": [
    "c1, c2,c3,c4= np.bincount(np.argmax(y_train.to_numpy(),axis=1))\n",
    "total = c1+c2+c3+c4\n",
    "print(f\"Total: {total}. C1 {c1}, c2 {c2}, c3 {c3}, c4 {c4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "def make_model(metrics=METRICS, output_bias=None):\n",
    "  if output_bias is not None:\n",
    "    output_bias = tf.keras.initializers.Constant(output_bias) # inicializar el bias de la ultima capa de manera sesgada para que todas las clases sean \"casi\" iguales\n",
    "  model = keras.Sequential([\n",
    "      keras.layers.Dense(\n",
    "          16, activation='relu',\n",
    "          input_shape=(17,)),\n",
    "      keras.layers.Dropout(0.5),\n",
    "      keras.layers.Dense(4, activation='sigmoid',\n",
    "                         bias_initializer=output_bias),\n",
    "  ])\n",
    "\n",
    "  model.compile(\n",
    "      optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "      loss=keras.losses.BinaryCrossentropy(),\n",
    "      metrics=metrics)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_prc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-math.log(1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_train,y_train,batch_size=BATCH_SIZE)\n",
    "print(f\"Loss: {results[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = [c1/total,c2/total,c3/total,c4/total]\n",
    "frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi = np.log(frequency)\n",
    "bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "from math import exp\n",
    "def eqn(x, frequency=frequency):\n",
    "  sum_exp = sum([exp(x_i) for x_i in x])\n",
    "  return [exp(x[i])/sum_exp - frequency[i] for i in range(len(frequency))]\n",
    "\n",
    "# calculate bias init\n",
    "bias_init = fsolve(func=eqn,\n",
    "                  x0=[0]*len(frequency),\n",
    "                  ).tolist()\n",
    "\n",
    "bias_init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(output_bias=bias_init)\n",
    "model.evaluate(X_train,y_train,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights')\n",
    "model.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "model.load_weights(initial_weights)\n",
    "model.layers[-1].bias.assign([0.0,0.0,0.0,0.0])\n",
    "zero_bias_history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    validation_data=(X_test, y_test), \n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(output_bias=bias_init)\n",
    "model.load_weights(initial_weights)\n",
    "careful_bias_history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    validation_data=(X_test, y_test), \n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, label, n):\n",
    "  # Use a log scale on y-axis to show the wide range of values.\n",
    "  plt.semilogy(history.epoch, history.history['loss'],\n",
    "               color=colors[n], label='Train ' + label)\n",
    "  plt.semilogy(history.epoch, history.history['val_loss'],\n",
    "               color=colors[n], label='Val ' + label,\n",
    "               linestyle=\"--\")\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(zero_bias_history, \"Zero Bias\", 0)\n",
    "plot_loss(careful_bias_history, \"Careful Bias\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "  metrics = ['loss', 'prc', 'precision', 'recall']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    if metric == 'loss':\n",
    "      plt.ylim([0, plt.ylim()[1]])\n",
    "    elif metric == 'auc':\n",
    "      plt.ylim([0.8,1])\n",
    "    else:\n",
    "      plt.ylim([0,1])\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(careful_bias_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_baseline = model.predict(X_train, batch_size=BATCH_SIZE)\n",
    "test_predictions_baseline = model.predict(X_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(labels, predictions, p=0.5):\n",
    "  cm = confusion_matrix(labels,predictions)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix')\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = model.evaluate(X_test, y_test,\n",
    "                                  batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model.metrics_names, baseline_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(np.argmax(y_test.to_numpy(),axis=1), test_predictions_baseline.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generación de un modelo simple con un perceptron de una capa y pocas neuronas para conocer un poco donde estamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(17,)),\n",
    "   \n",
    "    #keras.layers.Dense(10, activation='relu'),\n",
    "\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=METRICS)\n",
    "model.summary()\n",
    "h_nocapas = model.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando 1 capa y pocas neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(17,)),\n",
    "    keras.layers.Dense(10, activation='elu'),\n",
    "\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model.summary()\n",
    "h_unacapa =model.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando dos capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(17,)),\n",
    "    keras.layers.Dense(10, activation='elu'),\n",
    "    keras.layers.Dense(10, activation='elu'),\n",
    "\n",
    "\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model.summary()\n",
    "h_doscapas =model.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que a mas capas, mejores resultados. Verificación utilizando muchas neuronas por capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(17,)),\n",
    "    keras.layers.Dense(128, activation='elu'),\n",
    "    keras.layers.Dense(128, activation='elu'),\n",
    "\n",
    "\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model.summary()\n",
    "h_128128= model.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados mejores. Probamos con aún más neuronas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(17,)),\n",
    "    keras.layers.Dense(512, activation='elu'),\n",
    "    keras.layers.Dense(512, activation='elu'),\n",
    "\n",
    "\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model.summary()\n",
    "h_512512= model.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadir más neuronas auemnta el tiempo de entrenamiento ligeramente (CPU only) sin dar mejores resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(17,)),\n",
    "    keras.layers.Dense(512, activation='elu'),\n",
    "    keras.layers.Dense(512, activation='elu'),\n",
    "    keras.layers.Dense(512, activation='elu'),\n",
    "    keras.layers.Dense(512, activation='elu'),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model.summary()\n",
    "h_512x4= model.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo ahora es conseguir un modelo lo bastante grande que haga overfit a los datos y reducirlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(17,)),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model.summary()\n",
    "h_100x4x20= model.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test)) #menos epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como la precisión no parece estancarse lo dejamos durante muchas mas epocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(17,)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model.summary()\n",
    "h_100x4x100= model.fit(X_train,y_train,epochs=100,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos el overfit del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h_100x4x100.history['val_categorical_accuracy'])\n",
    "plt.plot(h_100x4x100.history['categorical_accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['val_accuracy','train_accuracy'], loc='upper left')\n",
    "plt.show()\n",
    "# \"Loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos estrategias de regularización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(17,)),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model.summary()\n",
    "h_100x4x100= model.fit(X_train,y_train,epochs=100,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiene buana pinta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(17,)),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model.summary()\n",
    "h_100x4x100= model.fit(X_train,y_train,epochs=100,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadir una capa mas no aporta nada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(17,)),\n",
    "    \n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.25),\n",
    "\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.25),\n",
    "\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model.summary()\n",
    "h_100x4x100= model.fit(X_train,y_train,epochs=100,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h_128128.history['val_categorical_accuracy'])\n",
    "plt.plot(h_nocapas.history['val_categorical_accuracy'])\n",
    "plt.plot(h_512512.history['val_categorical_accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['h28128', 'nocapas','512512'], loc='upper left')\n",
    "plt.show()\n",
    "# \"Loss\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons_ocultas=[30], learning_rate=3e-3, input_shape=(8,), \n",
    "                activation_o=\"relu\", optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=METRICS, \n",
    "                activation_s=\"soft_max\", n_neurons_salida=4):     \n",
    "    model = keras.models.Sequential()     \n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    if n_hidden != len(n_neurons_ocultas):\n",
    "        print(\"error, tamaño de la lista de n_neuronas no corresponde al umero de capas\")\n",
    "        return -1\n",
    "    for layer in range(n_hidden):         \n",
    "        model.add(keras.layers.Dense(n_neurons_ocultas[layer], activation=\"relu\"))     \n",
    "  \n",
    "    model.add(keras.layers.Dense(n_neurons_salida, activation=activation_s))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)     \n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=metrics)    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 30)                540       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 20)                620       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 84        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,244\n",
      "Trainable params: 1,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "51/51 [==============================] - 2s 14ms/step - loss: 0.9303 - tp: 10271.0000 - fp: 2929.0000 - tn: 59954.0000 - fn: 10690.0000 - accuracy: 0.8376 - precision: 0.7781 - recall: 0.4900 - auc: 0.8964 - prc: 0.7570 - val_loss: 0.7535 - val_tp: 922.0000 - val_fp: 418.0000 - val_tn: 4421.0000 - val_fn: 691.0000 - val_accuracy: 0.8281 - val_precision: 0.6881 - val_recall: 0.5716 - val_auc: 0.8964 - val_prc: 0.7512\n",
      "Epoch 2/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.6914 - tp: 8044.0000 - fp: 3327.0000 - tn: 35364.0000 - fn: 4853.0000 - accuracy: 0.8414 - precision: 0.7074 - recall: 0.6237 - auc: 0.9093 - prc: 0.7649 - val_loss: 0.7231 - val_tp: 1009.0000 - val_fp: 494.0000 - val_tn: 4345.0000 - val_fn: 604.0000 - val_accuracy: 0.8298 - val_precision: 0.6713 - val_recall: 0.6255 - val_auc: 0.9015 - val_prc: 0.7533\n",
      "Epoch 3/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.6186 - tp: 9011.0000 - fp: 3250.0000 - tn: 35441.0000 - fn: 3886.0000 - accuracy: 0.8617 - precision: 0.7349 - recall: 0.6987 - auc: 0.9275 - prc: 0.8068 - val_loss: 0.5397 - val_tp: 1211.0000 - val_fp: 357.0000 - val_tn: 4482.0000 - val_fn: 402.0000 - val_accuracy: 0.8824 - val_precision: 0.7723 - val_recall: 0.7508 - val_auc: 0.9452 - val_prc: 0.8522\n",
      "Epoch 4/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.5750 - tp: 9372.0000 - fp: 3162.0000 - tn: 35529.0000 - fn: 3525.0000 - accuracy: 0.8704 - precision: 0.7477 - recall: 0.7267 - auc: 0.9366 - prc: 0.8304 - val_loss: 0.7406 - val_tp: 1023.0000 - val_fp: 520.0000 - val_tn: 4319.0000 - val_fn: 590.0000 - val_accuracy: 0.8280 - val_precision: 0.6630 - val_recall: 0.6342 - val_auc: 0.9025 - val_prc: 0.7586\n",
      "Epoch 5/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.5514 - tp: 9611.0000 - fp: 2998.0000 - tn: 35693.0000 - fn: 3286.0000 - accuracy: 0.8782 - precision: 0.7622 - recall: 0.7452 - auc: 0.9422 - prc: 0.8444 - val_loss: 0.5198 - val_tp: 1236.0000 - val_fp: 351.0000 - val_tn: 4488.0000 - val_fn: 377.0000 - val_accuracy: 0.8872 - val_precision: 0.7788 - val_recall: 0.7663 - val_auc: 0.9488 - val_prc: 0.8622\n",
      "Epoch 6/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.5429 - tp: 9672.0000 - fp: 3005.0000 - tn: 35686.0000 - fn: 3225.0000 - accuracy: 0.8792 - precision: 0.7630 - recall: 0.7499 - auc: 0.9436 - prc: 0.8477 - val_loss: 0.5149 - val_tp: 1232.0000 - val_fp: 346.0000 - val_tn: 4493.0000 - val_fn: 381.0000 - val_accuracy: 0.8873 - val_precision: 0.7807 - val_recall: 0.7638 - val_auc: 0.9500 - val_prc: 0.8661\n",
      "Epoch 7/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.5232 - tp: 9800.0000 - fp: 2885.0000 - tn: 35806.0000 - fn: 3097.0000 - accuracy: 0.8840 - precision: 0.7726 - recall: 0.7599 - auc: 0.9470 - prc: 0.8549 - val_loss: 0.5955 - val_tp: 1183.0000 - val_fp: 421.0000 - val_tn: 4418.0000 - val_fn: 430.0000 - val_accuracy: 0.8681 - val_precision: 0.7375 - val_recall: 0.7334 - val_auc: 0.9319 - val_prc: 0.8116\n",
      "Epoch 8/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4880 - tp: 10053.0000 - fp: 2701.0000 - tn: 35990.0000 - fn: 2844.0000 - accuracy: 0.8925 - precision: 0.7882 - recall: 0.7795 - auc: 0.9541 - prc: 0.8743 - val_loss: 0.4886 - val_tp: 1252.0000 - val_fp: 343.0000 - val_tn: 4496.0000 - val_fn: 361.0000 - val_accuracy: 0.8909 - val_precision: 0.7850 - val_recall: 0.7762 - val_auc: 0.9543 - val_prc: 0.8756\n",
      "Epoch 9/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4909 - tp: 10061.0000 - fp: 2695.0000 - tn: 35996.0000 - fn: 2836.0000 - accuracy: 0.8928 - precision: 0.7887 - recall: 0.7801 - auc: 0.9535 - prc: 0.8726 - val_loss: 0.5369 - val_tp: 1220.0000 - val_fp: 372.0000 - val_tn: 4467.0000 - val_fn: 393.0000 - val_accuracy: 0.8814 - val_precision: 0.7663 - val_recall: 0.7564 - val_auc: 0.9454 - val_prc: 0.8548\n",
      "Epoch 10/100\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.4901 - tp: 10038.0000 - fp: 2725.0000 - tn: 35966.0000 - fn: 2859.0000 - accuracy: 0.8918 - precision: 0.7865 - recall: 0.7783 - auc: 0.9539 - prc: 0.8739 - val_loss: 0.5000 - val_tp: 1249.0000 - val_fp: 335.0000 - val_tn: 4504.0000 - val_fn: 364.0000 - val_accuracy: 0.8917 - val_precision: 0.7885 - val_recall: 0.7743 - val_auc: 0.9523 - val_prc: 0.8721\n",
      "Epoch 11/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4739 - tp: 10155.0000 - fp: 2606.0000 - tn: 36085.0000 - fn: 2742.0000 - accuracy: 0.8963 - precision: 0.7958 - recall: 0.7874 - auc: 0.9567 - prc: 0.8813 - val_loss: 0.4796 - val_tp: 1268.0000 - val_fp: 333.0000 - val_tn: 4506.0000 - val_fn: 345.0000 - val_accuracy: 0.8949 - val_precision: 0.7920 - val_recall: 0.7861 - val_auc: 0.9558 - val_prc: 0.8790\n",
      "Epoch 12/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4766 - tp: 10138.0000 - fp: 2611.0000 - tn: 36080.0000 - fn: 2759.0000 - accuracy: 0.8959 - precision: 0.7952 - recall: 0.7861 - auc: 0.9563 - prc: 0.8806 - val_loss: 0.4554 - val_tp: 1281.0000 - val_fp: 319.0000 - val_tn: 4520.0000 - val_fn: 332.0000 - val_accuracy: 0.8991 - val_precision: 0.8006 - val_recall: 0.7942 - val_auc: 0.9602 - val_prc: 0.8919\n",
      "Epoch 13/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4781 - tp: 10042.0000 - fp: 2711.0000 - tn: 35980.0000 - fn: 2855.0000 - accuracy: 0.8921 - precision: 0.7874 - recall: 0.7786 - auc: 0.9558 - prc: 0.8790 - val_loss: 0.5470 - val_tp: 1192.0000 - val_fp: 386.0000 - val_tn: 4453.0000 - val_fn: 421.0000 - val_accuracy: 0.8749 - val_precision: 0.7554 - val_recall: 0.7390 - val_auc: 0.9429 - val_prc: 0.8466\n",
      "Epoch 14/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4749 - tp: 10124.0000 - fp: 2647.0000 - tn: 36044.0000 - fn: 2773.0000 - accuracy: 0.8949 - precision: 0.7927 - recall: 0.7850 - auc: 0.9564 - prc: 0.8804 - val_loss: 0.4424 - val_tp: 1279.0000 - val_fp: 318.0000 - val_tn: 4521.0000 - val_fn: 334.0000 - val_accuracy: 0.8989 - val_precision: 0.8009 - val_recall: 0.7929 - val_auc: 0.9626 - val_prc: 0.8980\n",
      "Epoch 15/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4713 - tp: 10150.0000 - fp: 2642.0000 - tn: 36049.0000 - fn: 2747.0000 - accuracy: 0.8955 - precision: 0.7935 - recall: 0.7870 - auc: 0.9571 - prc: 0.8823 - val_loss: 0.4702 - val_tp: 1273.0000 - val_fp: 329.0000 - val_tn: 4510.0000 - val_fn: 340.0000 - val_accuracy: 0.8963 - val_precision: 0.7946 - val_recall: 0.7892 - val_auc: 0.9575 - val_prc: 0.8846\n",
      "Epoch 16/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4701 - tp: 10166.0000 - fp: 2593.0000 - tn: 36098.0000 - fn: 2731.0000 - accuracy: 0.8968 - precision: 0.7968 - recall: 0.7882 - auc: 0.9575 - prc: 0.8839 - val_loss: 0.4991 - val_tp: 1254.0000 - val_fp: 349.0000 - val_tn: 4490.0000 - val_fn: 359.0000 - val_accuracy: 0.8903 - val_precision: 0.7823 - val_recall: 0.7774 - val_auc: 0.9518 - val_prc: 0.8648\n",
      "Epoch 17/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4520 - tp: 10271.0000 - fp: 2512.0000 - tn: 36179.0000 - fn: 2626.0000 - accuracy: 0.9004 - precision: 0.8035 - recall: 0.7964 - auc: 0.9605 - prc: 0.8912 - val_loss: 0.4703 - val_tp: 1276.0000 - val_fp: 319.0000 - val_tn: 4520.0000 - val_fn: 337.0000 - val_accuracy: 0.8983 - val_precision: 0.8000 - val_recall: 0.7911 - val_auc: 0.9572 - val_prc: 0.8846\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4566 - tp: 10209.0000 - fp: 2545.0000 - tn: 36146.0000 - fn: 2688.0000 - accuracy: 0.8986 - precision: 0.8005 - recall: 0.7916 - auc: 0.9597 - prc: 0.8897 - val_loss: 0.4873 - val_tp: 1260.0000 - val_fp: 343.0000 - val_tn: 4496.0000 - val_fn: 353.0000 - val_accuracy: 0.8921 - val_precision: 0.7860 - val_recall: 0.7812 - val_auc: 0.9541 - val_prc: 0.8745\n",
      "Epoch 19/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4574 - tp: 10238.0000 - fp: 2545.0000 - tn: 36146.0000 - fn: 2659.0000 - accuracy: 0.8991 - precision: 0.8009 - recall: 0.7938 - auc: 0.9596 - prc: 0.8892 - val_loss: 0.6249 - val_tp: 1136.0000 - val_fp: 442.0000 - val_tn: 4397.0000 - val_fn: 477.0000 - val_accuracy: 0.8576 - val_precision: 0.7199 - val_recall: 0.7043 - val_auc: 0.9302 - val_prc: 0.8174\n",
      "Epoch 20/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4606 - tp: 10171.0000 - fp: 2597.0000 - tn: 36094.0000 - fn: 2726.0000 - accuracy: 0.8968 - precision: 0.7966 - recall: 0.7886 - auc: 0.9587 - prc: 0.8867 - val_loss: 0.5609 - val_tp: 1188.0000 - val_fp: 415.0000 - val_tn: 4424.0000 - val_fn: 425.0000 - val_accuracy: 0.8698 - val_precision: 0.7411 - val_recall: 0.7365 - val_auc: 0.9400 - val_prc: 0.8338\n",
      "Epoch 21/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4587 - tp: 10197.0000 - fp: 2580.0000 - tn: 36111.0000 - fn: 2700.0000 - accuracy: 0.8977 - precision: 0.7981 - recall: 0.7906 - auc: 0.9592 - prc: 0.8881 - val_loss: 0.4389 - val_tp: 1293.0000 - val_fp: 310.0000 - val_tn: 4529.0000 - val_fn: 320.0000 - val_accuracy: 0.9024 - val_precision: 0.8066 - val_recall: 0.8016 - val_auc: 0.9631 - val_prc: 0.8996\n",
      "Epoch 22/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4495 - tp: 10313.0000 - fp: 2463.0000 - tn: 36228.0000 - fn: 2584.0000 - accuracy: 0.9022 - precision: 0.8072 - recall: 0.7996 - auc: 0.9609 - prc: 0.8925 - val_loss: 0.4738 - val_tp: 1263.0000 - val_fp: 335.0000 - val_tn: 4504.0000 - val_fn: 350.0000 - val_accuracy: 0.8938 - val_precision: 0.7904 - val_recall: 0.7830 - val_auc: 0.9562 - val_prc: 0.8783\n",
      "Epoch 23/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4587 - tp: 10219.0000 - fp: 2557.0000 - tn: 36134.0000 - fn: 2678.0000 - accuracy: 0.8985 - precision: 0.7999 - recall: 0.7924 - auc: 0.9593 - prc: 0.8885 - val_loss: 0.5902 - val_tp: 1196.0000 - val_fp: 402.0000 - val_tn: 4437.0000 - val_fn: 417.0000 - val_accuracy: 0.8731 - val_precision: 0.7484 - val_recall: 0.7415 - val_auc: 0.9383 - val_prc: 0.8336\n",
      "Epoch 24/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4520 - tp: 10285.0000 - fp: 2503.0000 - tn: 36188.0000 - fn: 2612.0000 - accuracy: 0.9008 - precision: 0.8043 - recall: 0.7975 - auc: 0.9604 - prc: 0.8910 - val_loss: 0.4573 - val_tp: 1267.0000 - val_fp: 322.0000 - val_tn: 4517.0000 - val_fn: 346.0000 - val_accuracy: 0.8965 - val_precision: 0.7974 - val_recall: 0.7855 - val_auc: 0.9596 - val_prc: 0.8900\n",
      "Epoch 25/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4393 - tp: 10380.0000 - fp: 2413.0000 - tn: 36278.0000 - fn: 2517.0000 - accuracy: 0.9044 - precision: 0.8114 - recall: 0.8048 - auc: 0.9627 - prc: 0.8976 - val_loss: 0.4579 - val_tp: 1274.0000 - val_fp: 326.0000 - val_tn: 4513.0000 - val_fn: 339.0000 - val_accuracy: 0.8969 - val_precision: 0.7962 - val_recall: 0.7898 - val_auc: 0.9597 - val_prc: 0.8900\n",
      "Epoch 26/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4411 - tp: 10365.0000 - fp: 2423.0000 - tn: 36268.0000 - fn: 2532.0000 - accuracy: 0.9040 - precision: 0.8105 - recall: 0.8037 - auc: 0.9623 - prc: 0.8960 - val_loss: 0.4582 - val_tp: 1278.0000 - val_fp: 319.0000 - val_tn: 4520.0000 - val_fn: 335.0000 - val_accuracy: 0.8986 - val_precision: 0.8003 - val_recall: 0.7923 - val_auc: 0.9590 - val_prc: 0.8873\n",
      "Epoch 27/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4359 - tp: 10367.0000 - fp: 2426.0000 - tn: 36265.0000 - fn: 2530.0000 - accuracy: 0.9039 - precision: 0.8104 - recall: 0.8038 - auc: 0.9632 - prc: 0.8990 - val_loss: 0.4400 - val_tp: 1287.0000 - val_fp: 306.0000 - val_tn: 4533.0000 - val_fn: 326.0000 - val_accuracy: 0.9020 - val_precision: 0.8079 - val_recall: 0.7979 - val_auc: 0.9622 - val_prc: 0.8974\n",
      "Epoch 28/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4454 - tp: 10306.0000 - fp: 2496.0000 - tn: 36195.0000 - fn: 2591.0000 - accuracy: 0.9014 - precision: 0.8050 - recall: 0.7991 - auc: 0.9615 - prc: 0.8941 - val_loss: 0.4492 - val_tp: 1283.0000 - val_fp: 317.0000 - val_tn: 4522.0000 - val_fn: 330.0000 - val_accuracy: 0.8997 - val_precision: 0.8019 - val_recall: 0.7954 - val_auc: 0.9609 - val_prc: 0.8926\n",
      "Epoch 29/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4420 - tp: 10345.0000 - fp: 2442.0000 - tn: 36249.0000 - fn: 2552.0000 - accuracy: 0.9032 - precision: 0.8090 - recall: 0.8021 - auc: 0.9621 - prc: 0.8955 - val_loss: 0.4188 - val_tp: 1293.0000 - val_fp: 302.0000 - val_tn: 4537.0000 - val_fn: 320.0000 - val_accuracy: 0.9036 - val_precision: 0.8107 - val_recall: 0.8016 - val_auc: 0.9662 - val_prc: 0.9073\n",
      "Epoch 30/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4361 - tp: 10373.0000 - fp: 2428.0000 - tn: 36263.0000 - fn: 2524.0000 - accuracy: 0.9040 - precision: 0.8103 - recall: 0.8043 - auc: 0.9632 - prc: 0.8989 - val_loss: 0.4322 - val_tp: 1298.0000 - val_fp: 302.0000 - val_tn: 4537.0000 - val_fn: 315.0000 - val_accuracy: 0.9044 - val_precision: 0.8112 - val_recall: 0.8047 - val_auc: 0.9635 - val_prc: 0.9005\n",
      "Epoch 31/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4382 - tp: 10366.0000 - fp: 2443.0000 - tn: 36248.0000 - fn: 2531.0000 - accuracy: 0.9036 - precision: 0.8093 - recall: 0.8038 - auc: 0.9626 - prc: 0.8972 - val_loss: 0.4302 - val_tp: 1292.0000 - val_fp: 306.0000 - val_tn: 4533.0000 - val_fn: 321.0000 - val_accuracy: 0.9028 - val_precision: 0.8085 - val_recall: 0.8010 - val_auc: 0.9645 - val_prc: 0.9035\n",
      "Epoch 32/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4369 - tp: 10382.0000 - fp: 2422.0000 - tn: 36269.0000 - fn: 2515.0000 - accuracy: 0.9043 - precision: 0.8108 - recall: 0.8050 - auc: 0.9629 - prc: 0.8976 - val_loss: 0.5281 - val_tp: 1234.0000 - val_fp: 374.0000 - val_tn: 4465.0000 - val_fn: 379.0000 - val_accuracy: 0.8833 - val_precision: 0.7674 - val_recall: 0.7650 - val_auc: 0.9480 - val_prc: 0.8611\n",
      "Epoch 33/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4267 - tp: 10392.0000 - fp: 2403.0000 - tn: 36288.0000 - fn: 2505.0000 - accuracy: 0.9049 - precision: 0.8122 - recall: 0.8058 - auc: 0.9646 - prc: 0.9024 - val_loss: 0.4370 - val_tp: 1288.0000 - val_fp: 310.0000 - val_tn: 4529.0000 - val_fn: 325.0000 - val_accuracy: 0.9016 - val_precision: 0.8060 - val_recall: 0.7985 - val_auc: 0.9629 - val_prc: 0.8979\n",
      "Epoch 34/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4315 - tp: 10437.0000 - fp: 2365.0000 - tn: 36326.0000 - fn: 2460.0000 - accuracy: 0.9065 - precision: 0.8153 - recall: 0.8093 - auc: 0.9640 - prc: 0.9009 - val_loss: 0.4299 - val_tp: 1283.0000 - val_fp: 317.0000 - val_tn: 4522.0000 - val_fn: 330.0000 - val_accuracy: 0.8997 - val_precision: 0.8019 - val_recall: 0.7954 - val_auc: 0.9638 - val_prc: 0.9015\n",
      "Epoch 35/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4401 - tp: 10374.0000 - fp: 2424.0000 - tn: 36267.0000 - fn: 2523.0000 - accuracy: 0.9041 - precision: 0.8106 - recall: 0.8044 - auc: 0.9625 - prc: 0.8967 - val_loss: 0.4311 - val_tp: 1280.0000 - val_fp: 311.0000 - val_tn: 4528.0000 - val_fn: 333.0000 - val_accuracy: 0.9002 - val_precision: 0.8045 - val_recall: 0.7936 - val_auc: 0.9637 - val_prc: 0.8999\n",
      "Epoch 36/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4279 - tp: 10452.0000 - fp: 2342.0000 - tn: 36349.0000 - fn: 2445.0000 - accuracy: 0.9072 - precision: 0.8169 - recall: 0.8104 - auc: 0.9646 - prc: 0.9025 - val_loss: 0.4291 - val_tp: 1297.0000 - val_fp: 306.0000 - val_tn: 4533.0000 - val_fn: 316.0000 - val_accuracy: 0.9036 - val_precision: 0.8091 - val_recall: 0.8041 - val_auc: 0.9645 - val_prc: 0.9018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4300 - tp: 10405.0000 - fp: 2383.0000 - tn: 36308.0000 - fn: 2492.0000 - accuracy: 0.9055 - precision: 0.8137 - recall: 0.8068 - auc: 0.9639 - prc: 0.9004 - val_loss: 0.4378 - val_tp: 1290.0000 - val_fp: 302.0000 - val_tn: 4537.0000 - val_fn: 323.0000 - val_accuracy: 0.9031 - val_precision: 0.8103 - val_recall: 0.7998 - val_auc: 0.9626 - val_prc: 0.8969\n",
      "Epoch 38/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4368 - tp: 10362.0000 - fp: 2431.0000 - tn: 36260.0000 - fn: 2535.0000 - accuracy: 0.9037 - precision: 0.8100 - recall: 0.8034 - auc: 0.9630 - prc: 0.8978 - val_loss: 0.4641 - val_tp: 1267.0000 - val_fp: 341.0000 - val_tn: 4498.0000 - val_fn: 346.0000 - val_accuracy: 0.8935 - val_precision: 0.7879 - val_recall: 0.7855 - val_auc: 0.9581 - val_prc: 0.8839\n",
      "Epoch 39/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4214 - tp: 10465.0000 - fp: 2324.0000 - tn: 36367.0000 - fn: 2432.0000 - accuracy: 0.9078 - precision: 0.8183 - recall: 0.8114 - auc: 0.9655 - prc: 0.9050 - val_loss: 0.4172 - val_tp: 1302.0000 - val_fp: 294.0000 - val_tn: 4545.0000 - val_fn: 311.0000 - val_accuracy: 0.9062 - val_precision: 0.8158 - val_recall: 0.8072 - val_auc: 0.9668 - val_prc: 0.9092\n",
      "Epoch 40/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4129 - tp: 10535.0000 - fp: 2276.0000 - tn: 36415.0000 - fn: 2362.0000 - accuracy: 0.9101 - precision: 0.8223 - recall: 0.8169 - auc: 0.9669 - prc: 0.9090 - val_loss: 0.4322 - val_tp: 1293.0000 - val_fp: 306.0000 - val_tn: 4533.0000 - val_fn: 320.0000 - val_accuracy: 0.9030 - val_precision: 0.8086 - val_recall: 0.8016 - val_auc: 0.9637 - val_prc: 0.9012\n",
      "Epoch 41/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4213 - tp: 10455.0000 - fp: 2347.0000 - tn: 36344.0000 - fn: 2442.0000 - accuracy: 0.9072 - precision: 0.8167 - recall: 0.8107 - auc: 0.9656 - prc: 0.9055 - val_loss: 0.4515 - val_tp: 1285.0000 - val_fp: 315.0000 - val_tn: 4524.0000 - val_fn: 328.0000 - val_accuracy: 0.9003 - val_precision: 0.8031 - val_recall: 0.7967 - val_auc: 0.9606 - val_prc: 0.8929\n",
      "Epoch 42/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4191 - tp: 10501.0000 - fp: 2320.0000 - tn: 36371.0000 - fn: 2396.0000 - accuracy: 0.9086 - precision: 0.8190 - recall: 0.8142 - auc: 0.9659 - prc: 0.9062 - val_loss: 0.4656 - val_tp: 1269.0000 - val_fp: 334.0000 - val_tn: 4505.0000 - val_fn: 344.0000 - val_accuracy: 0.8949 - val_precision: 0.7916 - val_recall: 0.7867 - val_auc: 0.9581 - val_prc: 0.8862\n",
      "Epoch 43/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4278 - tp: 10414.0000 - fp: 2387.0000 - tn: 36304.0000 - fn: 2483.0000 - accuracy: 0.9056 - precision: 0.8135 - recall: 0.8075 - auc: 0.9644 - prc: 0.9019 - val_loss: 0.4238 - val_tp: 1306.0000 - val_fp: 291.0000 - val_tn: 4548.0000 - val_fn: 307.0000 - val_accuracy: 0.9073 - val_precision: 0.8178 - val_recall: 0.8097 - val_auc: 0.9651 - val_prc: 0.9044\n",
      "Epoch 44/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4259 - tp: 10483.0000 - fp: 2318.0000 - tn: 36373.0000 - fn: 2414.0000 - accuracy: 0.9083 - precision: 0.8189 - recall: 0.8128 - auc: 0.9648 - prc: 0.9031 - val_loss: 0.6368 - val_tp: 1149.0000 - val_fp: 451.0000 - val_tn: 4388.0000 - val_fn: 464.0000 - val_accuracy: 0.8582 - val_precision: 0.7181 - val_recall: 0.7123 - val_auc: 0.9300 - val_prc: 0.8163\n",
      "Epoch 45/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4218 - tp: 10498.0000 - fp: 2289.0000 - tn: 36402.0000 - fn: 2399.0000 - accuracy: 0.9091 - precision: 0.8210 - recall: 0.8140 - auc: 0.9655 - prc: 0.9048 - val_loss: 0.4197 - val_tp: 1311.0000 - val_fp: 294.0000 - val_tn: 4545.0000 - val_fn: 302.0000 - val_accuracy: 0.9076 - val_precision: 0.8168 - val_recall: 0.8128 - val_auc: 0.9658 - val_prc: 0.9060\n",
      "Epoch 46/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4299 - tp: 10456.0000 - fp: 2351.0000 - tn: 36340.0000 - fn: 2441.0000 - accuracy: 0.9071 - precision: 0.8164 - recall: 0.8107 - auc: 0.9642 - prc: 0.9013 - val_loss: 0.4445 - val_tp: 1272.0000 - val_fp: 311.0000 - val_tn: 4528.0000 - val_fn: 341.0000 - val_accuracy: 0.8989 - val_precision: 0.8035 - val_recall: 0.7886 - val_auc: 0.9615 - val_prc: 0.8953\n",
      "Epoch 47/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4160 - tp: 10555.0000 - fp: 2253.0000 - tn: 36438.0000 - fn: 2342.0000 - accuracy: 0.9109 - precision: 0.8241 - recall: 0.8184 - auc: 0.9663 - prc: 0.9070 - val_loss: 0.5283 - val_tp: 1232.0000 - val_fp: 375.0000 - val_tn: 4464.0000 - val_fn: 381.0000 - val_accuracy: 0.8828 - val_precision: 0.7666 - val_recall: 0.7638 - val_auc: 0.9486 - val_prc: 0.8592\n",
      "Epoch 48/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4273 - tp: 10466.0000 - fp: 2348.0000 - tn: 36343.0000 - fn: 2431.0000 - accuracy: 0.9074 - precision: 0.8168 - recall: 0.8115 - auc: 0.9646 - prc: 0.9024 - val_loss: 0.5021 - val_tp: 1236.0000 - val_fp: 365.0000 - val_tn: 4474.0000 - val_fn: 377.0000 - val_accuracy: 0.8850 - val_precision: 0.7720 - val_recall: 0.7663 - val_auc: 0.9517 - val_prc: 0.8685\n",
      "Epoch 49/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4197 - tp: 10475.0000 - fp: 2325.0000 - tn: 36366.0000 - fn: 2422.0000 - accuracy: 0.9080 - precision: 0.8184 - recall: 0.8122 - auc: 0.9657 - prc: 0.9053 - val_loss: 0.4804 - val_tp: 1255.0000 - val_fp: 347.0000 - val_tn: 4492.0000 - val_fn: 358.0000 - val_accuracy: 0.8907 - val_precision: 0.7834 - val_recall: 0.7781 - val_auc: 0.9556 - val_prc: 0.8792\n",
      "Epoch 50/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4155 - tp: 10504.0000 - fp: 2320.0000 - tn: 36371.0000 - fn: 2393.0000 - accuracy: 0.9086 - precision: 0.8191 - recall: 0.8145 - auc: 0.9665 - prc: 0.9076 - val_loss: 0.4253 - val_tp: 1306.0000 - val_fp: 296.0000 - val_tn: 4543.0000 - val_fn: 307.0000 - val_accuracy: 0.9065 - val_precision: 0.8152 - val_recall: 0.8097 - val_auc: 0.9648 - val_prc: 0.9038\n",
      "Epoch 51/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4174 - tp: 10528.0000 - fp: 2277.0000 - tn: 36414.0000 - fn: 2369.0000 - accuracy: 0.9099 - precision: 0.8222 - recall: 0.8163 - auc: 0.9662 - prc: 0.9066 - val_loss: 0.4327 - val_tp: 1306.0000 - val_fp: 295.0000 - val_tn: 4544.0000 - val_fn: 307.0000 - val_accuracy: 0.9067 - val_precision: 0.8157 - val_recall: 0.8097 - val_auc: 0.9640 - val_prc: 0.9027\n",
      "Epoch 52/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4111 - tp: 10528.0000 - fp: 2280.0000 - tn: 36411.0000 - fn: 2369.0000 - accuracy: 0.9099 - precision: 0.8220 - recall: 0.8163 - auc: 0.9671 - prc: 0.9092 - val_loss: 0.4210 - val_tp: 1304.0000 - val_fp: 297.0000 - val_tn: 4542.0000 - val_fn: 309.0000 - val_accuracy: 0.9061 - val_precision: 0.8145 - val_recall: 0.8084 - val_auc: 0.9655 - val_prc: 0.9054\n",
      "Epoch 53/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4200 - tp: 10505.0000 - fp: 2313.0000 - tn: 36378.0000 - fn: 2392.0000 - accuracy: 0.9088 - precision: 0.8196 - recall: 0.8145 - auc: 0.9657 - prc: 0.9053 - val_loss: 0.5075 - val_tp: 1252.0000 - val_fp: 343.0000 - val_tn: 4496.0000 - val_fn: 361.0000 - val_accuracy: 0.8909 - val_precision: 0.7850 - val_recall: 0.7762 - val_auc: 0.9525 - val_prc: 0.8734\n",
      "Epoch 54/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4183 - tp: 10496.0000 - fp: 2302.0000 - tn: 36389.0000 - fn: 2401.0000 - accuracy: 0.9088 - precision: 0.8201 - recall: 0.8138 - auc: 0.9660 - prc: 0.9066 - val_loss: 0.4660 - val_tp: 1278.0000 - val_fp: 314.0000 - val_tn: 4525.0000 - val_fn: 335.0000 - val_accuracy: 0.8994 - val_precision: 0.8028 - val_recall: 0.7923 - val_auc: 0.9583 - val_prc: 0.8868\n",
      "Epoch 55/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4091 - tp: 10581.0000 - fp: 2233.0000 - tn: 36458.0000 - fn: 2316.0000 - accuracy: 0.9118 - precision: 0.8257 - recall: 0.8204 - auc: 0.9675 - prc: 0.9101 - val_loss: 0.4553 - val_tp: 1273.0000 - val_fp: 324.0000 - val_tn: 4515.0000 - val_fn: 340.0000 - val_accuracy: 0.8971 - val_precision: 0.7971 - val_recall: 0.7892 - val_auc: 0.9598 - val_prc: 0.8909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4217 - tp: 10472.0000 - fp: 2335.0000 - tn: 36356.0000 - fn: 2425.0000 - accuracy: 0.9077 - precision: 0.8177 - recall: 0.8120 - auc: 0.9654 - prc: 0.9046 - val_loss: 0.4470 - val_tp: 1291.0000 - val_fp: 303.0000 - val_tn: 4536.0000 - val_fn: 322.0000 - val_accuracy: 0.9031 - val_precision: 0.8099 - val_recall: 0.8004 - val_auc: 0.9613 - val_prc: 0.8951\n",
      "Epoch 57/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4102 - tp: 10514.0000 - fp: 2283.0000 - tn: 36408.0000 - fn: 2383.0000 - accuracy: 0.9096 - precision: 0.8216 - recall: 0.8152 - auc: 0.9673 - prc: 0.9098 - val_loss: 0.4259 - val_tp: 1315.0000 - val_fp: 292.0000 - val_tn: 4547.0000 - val_fn: 298.0000 - val_accuracy: 0.9086 - val_precision: 0.8183 - val_recall: 0.8153 - val_auc: 0.9648 - val_prc: 0.9032\n",
      "Epoch 58/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4098 - tp: 10526.0000 - fp: 2290.0000 - tn: 36401.0000 - fn: 2371.0000 - accuracy: 0.9096 - precision: 0.8213 - recall: 0.8162 - auc: 0.9673 - prc: 0.9097 - val_loss: 0.4266 - val_tp: 1310.0000 - val_fp: 297.0000 - val_tn: 4542.0000 - val_fn: 303.0000 - val_accuracy: 0.9070 - val_precision: 0.8152 - val_recall: 0.8122 - val_auc: 0.9645 - val_prc: 0.9028\n",
      "Epoch 59/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4204 - tp: 10506.0000 - fp: 2295.0000 - tn: 36396.0000 - fn: 2391.0000 - accuracy: 0.9092 - precision: 0.8207 - recall: 0.8146 - auc: 0.9656 - prc: 0.9051 - val_loss: 0.4908 - val_tp: 1278.0000 - val_fp: 325.0000 - val_tn: 4514.0000 - val_fn: 335.0000 - val_accuracy: 0.8977 - val_precision: 0.7973 - val_recall: 0.7923 - val_auc: 0.9555 - val_prc: 0.8830\n",
      "Epoch 60/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4210 - tp: 10453.0000 - fp: 2349.0000 - tn: 36342.0000 - fn: 2444.0000 - accuracy: 0.9071 - precision: 0.8165 - recall: 0.8105 - auc: 0.9655 - prc: 0.9053 - val_loss: 0.4509 - val_tp: 1293.0000 - val_fp: 311.0000 - val_tn: 4528.0000 - val_fn: 320.0000 - val_accuracy: 0.9022 - val_precision: 0.8061 - val_recall: 0.8016 - val_auc: 0.9615 - val_prc: 0.8960\n",
      "Epoch 61/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4079 - tp: 10536.0000 - fp: 2271.0000 - tn: 36420.0000 - fn: 2361.0000 - accuracy: 0.9102 - precision: 0.8227 - recall: 0.8169 - auc: 0.9677 - prc: 0.9105 - val_loss: 0.4798 - val_tp: 1282.0000 - val_fp: 323.0000 - val_tn: 4516.0000 - val_fn: 331.0000 - val_accuracy: 0.8986 - val_precision: 0.7988 - val_recall: 0.7948 - val_auc: 0.9572 - val_prc: 0.8872\n",
      "Epoch 62/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4053 - tp: 10590.0000 - fp: 2236.0000 - tn: 36455.0000 - fn: 2307.0000 - accuracy: 0.9119 - precision: 0.8257 - recall: 0.8211 - auc: 0.9681 - prc: 0.9121 - val_loss: 0.4262 - val_tp: 1309.0000 - val_fp: 296.0000 - val_tn: 4543.0000 - val_fn: 304.0000 - val_accuracy: 0.9070 - val_precision: 0.8156 - val_recall: 0.8115 - val_auc: 0.9648 - val_prc: 0.9041\n",
      "Epoch 63/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4158 - tp: 10539.0000 - fp: 2273.0000 - tn: 36418.0000 - fn: 2358.0000 - accuracy: 0.9102 - precision: 0.8226 - recall: 0.8172 - auc: 0.9664 - prc: 0.9071 - val_loss: 0.4520 - val_tp: 1285.0000 - val_fp: 311.0000 - val_tn: 4528.0000 - val_fn: 328.0000 - val_accuracy: 0.9010 - val_precision: 0.8051 - val_recall: 0.7967 - val_auc: 0.9608 - val_prc: 0.8941\n",
      "Epoch 64/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4068 - tp: 10577.0000 - fp: 2207.0000 - tn: 36484.0000 - fn: 2320.0000 - accuracy: 0.9122 - precision: 0.8274 - recall: 0.8201 - auc: 0.9679 - prc: 0.9114 - val_loss: 0.4893 - val_tp: 1254.0000 - val_fp: 351.0000 - val_tn: 4488.0000 - val_fn: 359.0000 - val_accuracy: 0.8900 - val_precision: 0.7813 - val_recall: 0.7774 - val_auc: 0.9549 - val_prc: 0.8764\n",
      "Epoch 65/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4082 - tp: 10551.0000 - fp: 2260.0000 - tn: 36431.0000 - fn: 2346.0000 - accuracy: 0.9107 - precision: 0.8236 - recall: 0.8181 - auc: 0.9676 - prc: 0.9104 - val_loss: 0.5101 - val_tp: 1245.0000 - val_fp: 364.0000 - val_tn: 4475.0000 - val_fn: 368.0000 - val_accuracy: 0.8865 - val_precision: 0.7738 - val_recall: 0.7719 - val_auc: 0.9506 - val_prc: 0.8633\n",
      "Epoch 66/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4095 - tp: 10530.0000 - fp: 2281.0000 - tn: 36410.0000 - fn: 2367.0000 - accuracy: 0.9099 - precision: 0.8219 - recall: 0.8165 - auc: 0.9674 - prc: 0.9099 - val_loss: 0.4778 - val_tp: 1257.0000 - val_fp: 336.0000 - val_tn: 4503.0000 - val_fn: 356.0000 - val_accuracy: 0.8927 - val_precision: 0.7891 - val_recall: 0.7793 - val_auc: 0.9564 - val_prc: 0.8811\n",
      "Epoch 67/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4069 - tp: 10602.0000 - fp: 2199.0000 - tn: 36492.0000 - fn: 2295.0000 - accuracy: 0.9129 - precision: 0.8282 - recall: 0.8221 - auc: 0.9678 - prc: 0.9112 - val_loss: 0.4438 - val_tp: 1296.0000 - val_fp: 307.0000 - val_tn: 4532.0000 - val_fn: 317.0000 - val_accuracy: 0.9033 - val_precision: 0.8085 - val_recall: 0.8035 - val_auc: 0.9622 - val_prc: 0.8976\n",
      "Epoch 68/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4103 - tp: 10537.0000 - fp: 2267.0000 - tn: 36424.0000 - fn: 2360.0000 - accuracy: 0.9103 - precision: 0.8229 - recall: 0.8170 - auc: 0.9674 - prc: 0.9098 - val_loss: 0.4990 - val_tp: 1254.0000 - val_fp: 345.0000 - val_tn: 4494.0000 - val_fn: 359.0000 - val_accuracy: 0.8909 - val_precision: 0.7842 - val_recall: 0.7774 - val_auc: 0.9539 - val_prc: 0.8766\n",
      "Epoch 69/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4101 - tp: 10544.0000 - fp: 2276.0000 - tn: 36415.0000 - fn: 2353.0000 - accuracy: 0.9103 - precision: 0.8225 - recall: 0.8176 - auc: 0.9673 - prc: 0.9097 - val_loss: 0.4813 - val_tp: 1254.0000 - val_fp: 351.0000 - val_tn: 4488.0000 - val_fn: 359.0000 - val_accuracy: 0.8900 - val_precision: 0.7813 - val_recall: 0.7774 - val_auc: 0.9565 - val_prc: 0.8823\n",
      "Epoch 70/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4013 - tp: 10617.0000 - fp: 2196.0000 - tn: 36495.0000 - fn: 2280.0000 - accuracy: 0.9132 - precision: 0.8286 - recall: 0.8232 - auc: 0.9686 - prc: 0.9131 - val_loss: 0.4663 - val_tp: 1292.0000 - val_fp: 314.0000 - val_tn: 4525.0000 - val_fn: 321.0000 - val_accuracy: 0.9016 - val_precision: 0.8045 - val_recall: 0.8010 - val_auc: 0.9593 - val_prc: 0.8916\n",
      "Epoch 71/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4038 - tp: 10604.0000 - fp: 2181.0000 - tn: 36510.0000 - fn: 2293.0000 - accuracy: 0.9133 - precision: 0.8294 - recall: 0.8222 - auc: 0.9683 - prc: 0.9126 - val_loss: 0.4532 - val_tp: 1297.0000 - val_fp: 313.0000 - val_tn: 4526.0000 - val_fn: 316.0000 - val_accuracy: 0.9025 - val_precision: 0.8056 - val_recall: 0.8041 - val_auc: 0.9605 - val_prc: 0.8923\n",
      "Epoch 72/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4038 - tp: 10567.0000 - fp: 2242.0000 - tn: 36449.0000 - fn: 2330.0000 - accuracy: 0.9114 - precision: 0.8250 - recall: 0.8193 - auc: 0.9684 - prc: 0.9129 - val_loss: 0.4251 - val_tp: 1309.0000 - val_fp: 297.0000 - val_tn: 4542.0000 - val_fn: 304.0000 - val_accuracy: 0.9069 - val_precision: 0.8151 - val_recall: 0.8115 - val_auc: 0.9651 - val_prc: 0.9051\n",
      "Epoch 73/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4034 - tp: 10593.0000 - fp: 2226.0000 - tn: 36465.0000 - fn: 2304.0000 - accuracy: 0.9122 - precision: 0.8264 - recall: 0.8214 - auc: 0.9683 - prc: 0.9126 - val_loss: 0.4187 - val_tp: 1300.0000 - val_fp: 297.0000 - val_tn: 4542.0000 - val_fn: 313.0000 - val_accuracy: 0.9055 - val_precision: 0.8140 - val_recall: 0.8060 - val_auc: 0.9661 - val_prc: 0.9075\n",
      "Epoch 74/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4124 - tp: 10513.0000 - fp: 2294.0000 - tn: 36397.0000 - fn: 2384.0000 - accuracy: 0.9093 - precision: 0.8209 - recall: 0.8152 - auc: 0.9669 - prc: 0.9086 - val_loss: 0.4330 - val_tp: 1298.0000 - val_fp: 303.0000 - val_tn: 4536.0000 - val_fn: 315.0000 - val_accuracy: 0.9042 - val_precision: 0.8107 - val_recall: 0.8047 - val_auc: 0.9634 - val_prc: 0.9002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4017 - tp: 10624.0000 - fp: 2189.0000 - tn: 36502.0000 - fn: 2273.0000 - accuracy: 0.9135 - precision: 0.8292 - recall: 0.8238 - auc: 0.9687 - prc: 0.9134 - val_loss: 0.4418 - val_tp: 1300.0000 - val_fp: 301.0000 - val_tn: 4538.0000 - val_fn: 313.0000 - val_accuracy: 0.9048 - val_precision: 0.8120 - val_recall: 0.8060 - val_auc: 0.9627 - val_prc: 0.8979\n",
      "Epoch 76/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3988 - tp: 10613.0000 - fp: 2206.0000 - tn: 36485.0000 - fn: 2284.0000 - accuracy: 0.9130 - precision: 0.8279 - recall: 0.8229 - auc: 0.9691 - prc: 0.9145 - val_loss: 0.4186 - val_tp: 1321.0000 - val_fp: 285.0000 - val_tn: 4554.0000 - val_fn: 292.0000 - val_accuracy: 0.9106 - val_precision: 0.8225 - val_recall: 0.8190 - val_auc: 0.9664 - val_prc: 0.9087\n",
      "Epoch 77/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4033 - tp: 10606.0000 - fp: 2202.0000 - tn: 36489.0000 - fn: 2291.0000 - accuracy: 0.9129 - precision: 0.8281 - recall: 0.8224 - auc: 0.9684 - prc: 0.9124 - val_loss: 0.4560 - val_tp: 1286.0000 - val_fp: 316.0000 - val_tn: 4523.0000 - val_fn: 327.0000 - val_accuracy: 0.9003 - val_precision: 0.8027 - val_recall: 0.7973 - val_auc: 0.9605 - val_prc: 0.8941\n",
      "Epoch 78/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4031 - tp: 10555.0000 - fp: 2233.0000 - tn: 36458.0000 - fn: 2342.0000 - accuracy: 0.9113 - precision: 0.8254 - recall: 0.8184 - auc: 0.9685 - prc: 0.9128 - val_loss: 0.4701 - val_tp: 1291.0000 - val_fp: 310.0000 - val_tn: 4529.0000 - val_fn: 322.0000 - val_accuracy: 0.9020 - val_precision: 0.8064 - val_recall: 0.8004 - val_auc: 0.9588 - val_prc: 0.8904\n",
      "Epoch 79/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4052 - tp: 10573.0000 - fp: 2250.0000 - tn: 36441.0000 - fn: 2324.0000 - accuracy: 0.9113 - precision: 0.8245 - recall: 0.8198 - auc: 0.9681 - prc: 0.9122 - val_loss: 0.4608 - val_tp: 1275.0000 - val_fp: 329.0000 - val_tn: 4510.0000 - val_fn: 338.0000 - val_accuracy: 0.8966 - val_precision: 0.7949 - val_recall: 0.7905 - val_auc: 0.9590 - val_prc: 0.8873\n",
      "Epoch 80/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4028 - tp: 10619.0000 - fp: 2186.0000 - tn: 36505.0000 - fn: 2278.0000 - accuracy: 0.9135 - precision: 0.8293 - recall: 0.8234 - auc: 0.9684 - prc: 0.9125 - val_loss: 0.4296 - val_tp: 1307.0000 - val_fp: 298.0000 - val_tn: 4541.0000 - val_fn: 306.0000 - val_accuracy: 0.9064 - val_precision: 0.8143 - val_recall: 0.8103 - val_auc: 0.9647 - val_prc: 0.9052\n",
      "Epoch 81/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4041 - tp: 10610.0000 - fp: 2208.0000 - tn: 36483.0000 - fn: 2287.0000 - accuracy: 0.9129 - precision: 0.8277 - recall: 0.8227 - auc: 0.9683 - prc: 0.9123 - val_loss: 0.4235 - val_tp: 1305.0000 - val_fp: 300.0000 - val_tn: 4539.0000 - val_fn: 308.0000 - val_accuracy: 0.9058 - val_precision: 0.8131 - val_recall: 0.8091 - val_auc: 0.9654 - val_prc: 0.9065\n",
      "Epoch 82/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3974 - tp: 10634.0000 - fp: 2175.0000 - tn: 36516.0000 - fn: 2263.0000 - accuracy: 0.9140 - precision: 0.8302 - recall: 0.8245 - auc: 0.9693 - prc: 0.9150 - val_loss: 0.4277 - val_tp: 1303.0000 - val_fp: 295.0000 - val_tn: 4544.0000 - val_fn: 310.0000 - val_accuracy: 0.9062 - val_precision: 0.8154 - val_recall: 0.8078 - val_auc: 0.9645 - val_prc: 0.9034\n",
      "Epoch 83/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4012 - tp: 10603.0000 - fp: 2206.0000 - tn: 36485.0000 - fn: 2294.0000 - accuracy: 0.9128 - precision: 0.8278 - recall: 0.8221 - auc: 0.9687 - prc: 0.9136 - val_loss: 0.4258 - val_tp: 1308.0000 - val_fp: 291.0000 - val_tn: 4548.0000 - val_fn: 305.0000 - val_accuracy: 0.9076 - val_precision: 0.8180 - val_recall: 0.8109 - val_auc: 0.9652 - val_prc: 0.9063\n",
      "Epoch 84/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3961 - tp: 10635.0000 - fp: 2174.0000 - tn: 36517.0000 - fn: 2262.0000 - accuracy: 0.9140 - precision: 0.8303 - recall: 0.8246 - auc: 0.9695 - prc: 0.9155 - val_loss: 0.4302 - val_tp: 1305.0000 - val_fp: 301.0000 - val_tn: 4538.0000 - val_fn: 308.0000 - val_accuracy: 0.9056 - val_precision: 0.8126 - val_recall: 0.8091 - val_auc: 0.9644 - val_prc: 0.9037\n",
      "Epoch 85/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3990 - tp: 10638.0000 - fp: 2173.0000 - tn: 36518.0000 - fn: 2259.0000 - accuracy: 0.9141 - precision: 0.8304 - recall: 0.8248 - auc: 0.9690 - prc: 0.9145 - val_loss: 0.4223 - val_tp: 1307.0000 - val_fp: 296.0000 - val_tn: 4543.0000 - val_fn: 306.0000 - val_accuracy: 0.9067 - val_precision: 0.8153 - val_recall: 0.8103 - val_auc: 0.9656 - val_prc: 0.9068\n",
      "Epoch 86/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3935 - tp: 10654.0000 - fp: 2156.0000 - tn: 36535.0000 - fn: 2243.0000 - accuracy: 0.9147 - precision: 0.8317 - recall: 0.8261 - auc: 0.9698 - prc: 0.9166 - val_loss: 0.5019 - val_tp: 1253.0000 - val_fp: 345.0000 - val_tn: 4494.0000 - val_fn: 360.0000 - val_accuracy: 0.8907 - val_precision: 0.7841 - val_recall: 0.7768 - val_auc: 0.9539 - val_prc: 0.8757\n",
      "Epoch 87/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3955 - tp: 10656.0000 - fp: 2156.0000 - tn: 36535.0000 - fn: 2241.0000 - accuracy: 0.9148 - precision: 0.8317 - recall: 0.8262 - auc: 0.9695 - prc: 0.9157 - val_loss: 0.4369 - val_tp: 1299.0000 - val_fp: 302.0000 - val_tn: 4537.0000 - val_fn: 314.0000 - val_accuracy: 0.9045 - val_precision: 0.8114 - val_recall: 0.8053 - val_auc: 0.9637 - val_prc: 0.9017\n",
      "Epoch 88/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4043 - tp: 10577.0000 - fp: 2221.0000 - tn: 36470.0000 - fn: 2320.0000 - accuracy: 0.9120 - precision: 0.8265 - recall: 0.8201 - auc: 0.9681 - prc: 0.9114 - val_loss: 0.4371 - val_tp: 1303.0000 - val_fp: 300.0000 - val_tn: 4539.0000 - val_fn: 310.0000 - val_accuracy: 0.9055 - val_precision: 0.8129 - val_recall: 0.8078 - val_auc: 0.9634 - val_prc: 0.9011\n",
      "Epoch 89/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3945 - tp: 10618.0000 - fp: 2194.0000 - tn: 36497.0000 - fn: 2279.0000 - accuracy: 0.9133 - precision: 0.8288 - recall: 0.8233 - auc: 0.9697 - prc: 0.9164 - val_loss: 0.4544 - val_tp: 1280.0000 - val_fp: 323.0000 - val_tn: 4516.0000 - val_fn: 333.0000 - val_accuracy: 0.8983 - val_precision: 0.7985 - val_recall: 0.7936 - val_auc: 0.9600 - val_prc: 0.8908\n",
      "Epoch 90/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3996 - tp: 10572.0000 - fp: 2218.0000 - tn: 36473.0000 - fn: 2325.0000 - accuracy: 0.9119 - precision: 0.8266 - recall: 0.8197 - auc: 0.9690 - prc: 0.9142 - val_loss: 0.4377 - val_tp: 1294.0000 - val_fp: 311.0000 - val_tn: 4528.0000 - val_fn: 319.0000 - val_accuracy: 0.9024 - val_precision: 0.8062 - val_recall: 0.8022 - val_auc: 0.9632 - val_prc: 0.9000\n",
      "Epoch 91/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3929 - tp: 10648.0000 - fp: 2161.0000 - tn: 36530.0000 - fn: 2249.0000 - accuracy: 0.9145 - precision: 0.8313 - recall: 0.8256 - auc: 0.9699 - prc: 0.9170 - val_loss: 0.4526 - val_tp: 1280.0000 - val_fp: 322.0000 - val_tn: 4517.0000 - val_fn: 333.0000 - val_accuracy: 0.8985 - val_precision: 0.7990 - val_recall: 0.7936 - val_auc: 0.9608 - val_prc: 0.8931\n",
      "Epoch 92/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3962 - tp: 10646.0000 - fp: 2166.0000 - tn: 36525.0000 - fn: 2251.0000 - accuracy: 0.9144 - precision: 0.8309 - recall: 0.8255 - auc: 0.9695 - prc: 0.9158 - val_loss: 0.4261 - val_tp: 1316.0000 - val_fp: 285.0000 - val_tn: 4554.0000 - val_fn: 297.0000 - val_accuracy: 0.9098 - val_precision: 0.8220 - val_recall: 0.8159 - val_auc: 0.9651 - val_prc: 0.9052\n",
      "Epoch 93/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3927 - tp: 10643.0000 - fp: 2170.0000 - tn: 36521.0000 - fn: 2254.0000 - accuracy: 0.9142 - precision: 0.8306 - recall: 0.8252 - auc: 0.9701 - prc: 0.9173 - val_loss: 0.4310 - val_tp: 1314.0000 - val_fp: 290.0000 - val_tn: 4549.0000 - val_fn: 299.0000 - val_accuracy: 0.9087 - val_precision: 0.8192 - val_recall: 0.8146 - val_auc: 0.9640 - val_prc: 0.9014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3966 - tp: 10646.0000 - fp: 2181.0000 - tn: 36510.0000 - fn: 2251.0000 - accuracy: 0.9141 - precision: 0.8300 - recall: 0.8255 - auc: 0.9693 - prc: 0.9153 - val_loss: 0.4295 - val_tp: 1306.0000 - val_fp: 295.0000 - val_tn: 4544.0000 - val_fn: 307.0000 - val_accuracy: 0.9067 - val_precision: 0.8157 - val_recall: 0.8097 - val_auc: 0.9647 - val_prc: 0.9048\n",
      "Epoch 95/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3975 - tp: 10602.0000 - fp: 2210.0000 - tn: 36481.0000 - fn: 2295.0000 - accuracy: 0.9127 - precision: 0.8275 - recall: 0.8221 - auc: 0.9693 - prc: 0.9149 - val_loss: 0.4669 - val_tp: 1277.0000 - val_fp: 316.0000 - val_tn: 4523.0000 - val_fn: 336.0000 - val_accuracy: 0.8989 - val_precision: 0.8016 - val_recall: 0.7917 - val_auc: 0.9595 - val_prc: 0.8913\n",
      "Epoch 96/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3909 - tp: 10621.0000 - fp: 2186.0000 - tn: 36505.0000 - fn: 2276.0000 - accuracy: 0.9135 - precision: 0.8293 - recall: 0.8235 - auc: 0.9702 - prc: 0.9175 - val_loss: 0.4351 - val_tp: 1303.0000 - val_fp: 302.0000 - val_tn: 4537.0000 - val_fn: 310.0000 - val_accuracy: 0.9051 - val_precision: 0.8118 - val_recall: 0.8078 - val_auc: 0.9634 - val_prc: 0.9006\n",
      "Epoch 97/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3887 - tp: 10679.0000 - fp: 2125.0000 - tn: 36566.0000 - fn: 2218.0000 - accuracy: 0.9158 - precision: 0.8340 - recall: 0.8280 - auc: 0.9706 - prc: 0.9188 - val_loss: 0.4465 - val_tp: 1297.0000 - val_fp: 307.0000 - val_tn: 4532.0000 - val_fn: 316.0000 - val_accuracy: 0.9034 - val_precision: 0.8086 - val_recall: 0.8041 - val_auc: 0.9618 - val_prc: 0.8955\n",
      "Epoch 98/100\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3888 - tp: 10703.0000 - fp: 2112.0000 - tn: 36579.0000 - fn: 2194.0000 - accuracy: 0.9165 - precision: 0.8352 - recall: 0.8299 - auc: 0.9706 - prc: 0.9187 - val_loss: 0.4299 - val_tp: 1294.0000 - val_fp: 307.0000 - val_tn: 4532.0000 - val_fn: 319.0000 - val_accuracy: 0.9030 - val_precision: 0.8082 - val_recall: 0.8022 - val_auc: 0.9646 - val_prc: 0.9040\n",
      "Epoch 99/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3912 - tp: 10682.0000 - fp: 2148.0000 - tn: 36543.0000 - fn: 2215.0000 - accuracy: 0.9154 - precision: 0.8326 - recall: 0.8283 - auc: 0.9703 - prc: 0.9177 - val_loss: 0.4257 - val_tp: 1312.0000 - val_fp: 297.0000 - val_tn: 4542.0000 - val_fn: 301.0000 - val_accuracy: 0.9073 - val_precision: 0.8154 - val_recall: 0.8134 - val_auc: 0.9651 - val_prc: 0.9046\n",
      "Epoch 100/100\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3935 - tp: 10640.0000 - fp: 2182.0000 - tn: 36509.0000 - fn: 2257.0000 - accuracy: 0.9140 - precision: 0.8298 - recall: 0.8250 - auc: 0.9698 - prc: 0.9160 - val_loss: 0.4731 - val_tp: 1284.0000 - val_fp: 314.0000 - val_tn: 4525.0000 - val_fn: 329.0000 - val_accuracy: 0.9003 - val_precision: 0.8035 - val_recall: 0.7960 - val_auc: 0.9591 - val_prc: 0.8895\n"
     ]
    }
   ],
   "source": [
    "model = build_model(2, [30,20], 0.5, (17,), \"relu\", \"adam\", \"categorical_crossentropy\", METRICS, \"softmax\", 4)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "h_100x4x100= model.fit(X_train,y_train,epochs=100,validation_data=(X_test,y_test), batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IGNORAR NLP CON TORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att.columns[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cadenas = []\n",
    "cadenas_target = []\n",
    "for x,y in zip(att.to_numpy(),target.to_numpy()):\n",
    "    temp = ['A football player with']\n",
    "    for i in range(0,len(x)):\n",
    "        temp.append(f' {att.columns[i]} of '+str(x[i])+',')\n",
    "    cadenas.append((''.join(temp)[:-1]))\n",
    "    cadenas_target.append(np.argmax(y,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cadenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = []\n",
    "for x,y in zip(cadenas,cadenas_target):\n",
    "\n",
    "\n",
    "    misdatos  = {}\n",
    "    misdatos['label'] = y\n",
    "    misdatos['text'] = x\n",
    "    datos.append(misdatos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=np.reshape(cadenas,(-1,1)),columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = np.reshape(cadenas_target,(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds = ds.with_format('torch')\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds['train'],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8863d0c6094564aaeb41a5a3a6a2d4ce9faecc1a6add5ccddbb72eef631d0103"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
